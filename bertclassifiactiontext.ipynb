{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QYUh6k3Mg9b",
        "outputId": "6e7634dd-8438-4f71-acc1-16308b3e9abb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "uOtXOj4XNLcr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Sxc-BATJOmwT",
        "outputId": "936fd7ac-03ea-4344-e5f2-80f4418f5829"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250520_175927-va8ls42n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kenkleven50-lbv/news-subject-classification/runs/va8ls42n' target=\"_blank\">bert-base-title-text</a></strong> to <a href='https://wandb.ai/kenkleven50-lbv/news-subject-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kenkleven50-lbv/news-subject-classification' target=\"_blank\">https://wandb.ai/kenkleven50-lbv/news-subject-classification</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kenkleven50-lbv/news-subject-classification/runs/va8ls42n' target=\"_blank\">https://wandb.ai/kenkleven50-lbv/news-subject-classification/runs/va8ls42n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.init(project=\"news-subject-classification\", name=\"bert-base-title-text\", config={\n",
        "    \"model\": \"bert-base-uncased\",\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs\": 3,\n",
        "    \"lr\": 2e-5,\n",
        "    \"max_len\": 512\n",
        "})\n",
        "config = wandb.config\n",
        "\n",
        "df = pd.read_csv(\"/content/True.csv\")\n",
        "df['text'] = df['title'] + \" \" + df['text']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['subject'])\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "YQtiI4jqSe5-",
        "outputId": "39923b05-4b7d-4c3b-d80c-5195526a747d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "0    As U.S. budget fight looms, Republicans flip t...\n",
              "1    U.S. military to accept transgender recruits o...\n",
              "2    Senior U.S. Republican senator: 'Let Mr. Muell...\n",
              "3    FBI Russia probe helped by Australian diplomat...\n",
              "4    Trump wants Postal Service to charge 'much mor...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "UhNmK9WaSdWf",
        "outputId": "e13bdc83-159a-41d8-c790-9bebee6f0190"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subject</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>politicsNews</th>\n",
              "      <td>11272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worldnews</th>\n",
              "      <td>10145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "subject\n",
              "politicsNews    11272\n",
              "worldnews       10145\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['subject'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbGQBd9fZYg9"
      },
      "outputs": [],
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = NewsDataset(train_texts, train_labels, tokenizer, config.max_len)\n",
        "test_dataset = NewsDataset(test_texts, test_labels, tokenizer, config.max_len)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0DOjD8NZcOe"
      },
      "outputs": [],
      "source": [
        "class NewsClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(NewsClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(config.model)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.fc(self.drop(pooled_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dt2JGpVQTSm",
        "outputId": "f23f7604-3601-4e15-d71c-027d3e4cb644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🟢 Début Epoch 1\n",
            "Step 0/2142\n",
            "Step 10/2142\n",
            "Step 20/2142\n",
            "Step 30/2142\n",
            "Step 40/2142\n",
            "Step 50/2142\n",
            "Step 60/2142\n",
            "Step 70/2142\n",
            "Step 80/2142\n",
            "Step 90/2142\n",
            "Step 100/2142\n",
            "Step 110/2142\n",
            "Step 120/2142\n",
            "Step 130/2142\n",
            "Step 140/2142\n",
            "Step 150/2142\n",
            "Step 160/2142\n",
            "Step 170/2142\n",
            "Step 180/2142\n",
            "Step 190/2142\n",
            "Step 200/2142\n",
            "Step 210/2142\n",
            "Step 220/2142\n",
            "Step 230/2142\n",
            "Step 240/2142\n",
            "Step 250/2142\n",
            "Step 260/2142\n",
            "Step 270/2142\n",
            "Step 280/2142\n",
            "Step 290/2142\n",
            "Step 300/2142\n",
            "Step 310/2142\n",
            "Step 320/2142\n",
            "Step 330/2142\n",
            "Step 340/2142\n",
            "Step 350/2142\n",
            "Step 360/2142\n",
            "Step 370/2142\n",
            "Step 380/2142\n",
            "Step 390/2142\n",
            "Step 400/2142\n",
            "Step 410/2142\n",
            "Step 420/2142\n",
            "Step 430/2142\n",
            "Step 440/2142\n",
            "Step 450/2142\n",
            "Step 460/2142\n",
            "Step 470/2142\n",
            "Step 480/2142\n",
            "Step 490/2142\n",
            "Step 500/2142\n",
            "Step 510/2142\n",
            "Step 520/2142\n",
            "Step 530/2142\n",
            "Step 540/2142\n",
            "Step 550/2142\n",
            "Step 560/2142\n",
            "Step 570/2142\n",
            "Step 580/2142\n",
            "Step 590/2142\n",
            "Step 600/2142\n",
            "Step 610/2142\n",
            "Step 620/2142\n",
            "Step 630/2142\n",
            "Step 640/2142\n",
            "Step 650/2142\n",
            "Step 660/2142\n",
            "Step 670/2142\n",
            "Step 680/2142\n",
            "Step 690/2142\n",
            "Step 700/2142\n",
            "Step 710/2142\n",
            "Step 720/2142\n",
            "Step 730/2142\n",
            "Step 740/2142\n",
            "Step 750/2142\n",
            "Step 760/2142\n",
            "Step 770/2142\n",
            "Step 780/2142\n",
            "Step 790/2142\n",
            "Step 800/2142\n",
            "Step 810/2142\n",
            "Step 820/2142\n",
            "Step 830/2142\n",
            "Step 840/2142\n",
            "Step 850/2142\n",
            "Step 860/2142\n",
            "Step 870/2142\n",
            "Step 880/2142\n",
            "Step 890/2142\n",
            "Step 900/2142\n",
            "Step 910/2142\n",
            "Step 920/2142\n",
            "Step 930/2142\n",
            "Step 940/2142\n",
            "Step 950/2142\n",
            "Step 960/2142\n",
            "Step 970/2142\n",
            "Step 980/2142\n",
            "Step 990/2142\n",
            "Step 1000/2142\n",
            "Step 1010/2142\n",
            "Step 1020/2142\n",
            "Step 1030/2142\n",
            "Step 1040/2142\n",
            "Step 1050/2142\n",
            "Step 1060/2142\n",
            "Step 1070/2142\n",
            "Step 1080/2142\n",
            "Step 1090/2142\n",
            "Step 1100/2142\n",
            "Step 1110/2142\n",
            "Step 1120/2142\n",
            "Step 1130/2142\n",
            "Step 1140/2142\n",
            "Step 1150/2142\n",
            "Step 1160/2142\n",
            "Step 1170/2142\n",
            "Step 1180/2142\n",
            "Step 1190/2142\n",
            "Step 1200/2142\n",
            "Step 1210/2142\n",
            "Step 1220/2142\n",
            "Step 1230/2142\n",
            "Step 1240/2142\n",
            "Step 1250/2142\n",
            "Step 1260/2142\n",
            "Step 1270/2142\n",
            "Step 1280/2142\n",
            "Step 1290/2142\n",
            "Step 1300/2142\n",
            "Step 1310/2142\n",
            "Step 1320/2142\n",
            "Step 1330/2142\n",
            "Step 1340/2142\n",
            "Step 1350/2142\n",
            "Step 1360/2142\n",
            "Step 1370/2142\n",
            "Step 1380/2142\n",
            "Step 1390/2142\n",
            "Step 1400/2142\n",
            "Step 1410/2142\n",
            "Step 1420/2142\n",
            "Step 1430/2142\n",
            "Step 1440/2142\n",
            "Step 1450/2142\n",
            "Step 1460/2142\n",
            "Step 1470/2142\n",
            "Step 1480/2142\n",
            "Step 1490/2142\n",
            "Step 1500/2142\n",
            "Step 1510/2142\n",
            "Step 1520/2142\n",
            "Step 1530/2142\n",
            "Step 1540/2142\n",
            "Step 1550/2142\n",
            "Step 1560/2142\n",
            "Step 1570/2142\n",
            "Step 1580/2142\n",
            "Step 1590/2142\n",
            "Step 1600/2142\n",
            "Step 1610/2142\n",
            "Step 1620/2142\n",
            "Step 1630/2142\n",
            "Step 1640/2142\n",
            "Step 1650/2142\n",
            "Step 1660/2142\n",
            "Step 1670/2142\n",
            "Step 1680/2142\n",
            "Step 1690/2142\n",
            "Step 1700/2142\n",
            "Step 1710/2142\n",
            "Step 1720/2142\n",
            "Step 1730/2142\n",
            "Step 1740/2142\n",
            "Step 1750/2142\n",
            "Step 1760/2142\n",
            "Step 1770/2142\n",
            "Step 1780/2142\n",
            "Step 1790/2142\n",
            "Step 1800/2142\n",
            "Step 1810/2142\n",
            "Step 1820/2142\n",
            "Step 1830/2142\n",
            "Step 1840/2142\n",
            "Step 1850/2142\n",
            "Step 1860/2142\n",
            "Step 1870/2142\n",
            "Step 1880/2142\n",
            "Step 1890/2142\n",
            "Step 1900/2142\n",
            "Step 1910/2142\n",
            "Step 1920/2142\n",
            "Step 1930/2142\n",
            "Step 1940/2142\n",
            "Step 1950/2142\n",
            "Step 1960/2142\n",
            "Step 1970/2142\n",
            "Step 1980/2142\n",
            "Step 1990/2142\n",
            "Step 2000/2142\n",
            "Step 2010/2142\n",
            "Step 2020/2142\n",
            "Step 2030/2142\n",
            "Step 2040/2142\n",
            "Step 2050/2142\n",
            "Step 2060/2142\n",
            "Step 2070/2142\n",
            "Step 2080/2142\n",
            "Step 2090/2142\n",
            "Step 2100/2142\n",
            "Step 2110/2142\n",
            "Step 2120/2142\n",
            "Step 2130/2142\n",
            "Step 2140/2142\n",
            "Epoch 1: Loss=71.6160, Accuracy=0.9887\n",
            "\n",
            "🟢 Début Epoch 2\n",
            "Step 0/2142\n",
            "Step 10/2142\n",
            "Step 20/2142\n",
            "Step 30/2142\n",
            "Step 40/2142\n",
            "Step 50/2142\n",
            "Step 60/2142\n",
            "Step 70/2142\n",
            "Step 80/2142\n",
            "Step 90/2142\n",
            "Step 100/2142\n",
            "Step 110/2142\n",
            "Step 120/2142\n",
            "Step 130/2142\n",
            "Step 140/2142\n",
            "Step 150/2142\n",
            "Step 160/2142\n",
            "Step 170/2142\n",
            "Step 180/2142\n",
            "Step 190/2142\n",
            "Step 200/2142\n",
            "Step 210/2142\n",
            "Step 220/2142\n",
            "Step 230/2142\n",
            "Step 240/2142\n",
            "Step 250/2142\n",
            "Step 260/2142\n",
            "Step 270/2142\n",
            "Step 280/2142\n",
            "Step 290/2142\n",
            "Step 300/2142\n",
            "Step 310/2142\n",
            "Step 320/2142\n",
            "Step 330/2142\n",
            "Step 340/2142\n",
            "Step 350/2142\n",
            "Step 360/2142\n",
            "Step 370/2142\n",
            "Step 380/2142\n",
            "Step 390/2142\n",
            "Step 400/2142\n",
            "Step 410/2142\n",
            "Step 420/2142\n",
            "Step 430/2142\n",
            "Step 440/2142\n",
            "Step 450/2142\n",
            "Step 460/2142\n",
            "Step 470/2142\n",
            "Step 480/2142\n",
            "Step 490/2142\n",
            "Step 500/2142\n",
            "Step 510/2142\n",
            "Step 520/2142\n",
            "Step 530/2142\n",
            "Step 540/2142\n",
            "Step 550/2142\n",
            "Step 560/2142\n",
            "Step 570/2142\n",
            "Step 580/2142\n",
            "Step 590/2142\n",
            "Step 600/2142\n",
            "Step 610/2142\n",
            "Step 620/2142\n",
            "Step 630/2142\n",
            "Step 640/2142\n",
            "Step 650/2142\n",
            "Step 660/2142\n",
            "Step 670/2142\n",
            "Step 680/2142\n",
            "Step 690/2142\n",
            "Step 700/2142\n",
            "Step 710/2142\n",
            "Step 720/2142\n",
            "Step 730/2142\n",
            "Step 740/2142\n",
            "Step 750/2142\n",
            "Step 760/2142\n",
            "Step 770/2142\n",
            "Step 780/2142\n",
            "Step 790/2142\n",
            "Step 800/2142\n",
            "Step 810/2142\n",
            "Step 820/2142\n",
            "Step 830/2142\n",
            "Step 840/2142\n",
            "Step 850/2142\n",
            "Step 860/2142\n",
            "Step 870/2142\n",
            "Step 880/2142\n",
            "Step 890/2142\n",
            "Step 900/2142\n",
            "Step 910/2142\n",
            "Step 920/2142\n",
            "Step 930/2142\n",
            "Step 940/2142\n",
            "Step 950/2142\n",
            "Step 960/2142\n",
            "Step 970/2142\n",
            "Step 980/2142\n",
            "Step 990/2142\n",
            "Step 1000/2142\n",
            "Step 1010/2142\n",
            "Step 1020/2142\n",
            "Step 1030/2142\n",
            "Step 1040/2142\n",
            "Step 1050/2142\n",
            "Step 1060/2142\n",
            "Step 1070/2142\n",
            "Step 1080/2142\n",
            "Step 1090/2142\n",
            "Step 1100/2142\n",
            "Step 1110/2142\n",
            "Step 1120/2142\n",
            "Step 1130/2142\n",
            "Step 1140/2142\n",
            "Step 1150/2142\n",
            "Step 1160/2142\n",
            "Step 1170/2142\n",
            "Step 1180/2142\n",
            "Step 1190/2142\n",
            "Step 1200/2142\n",
            "Step 1210/2142\n",
            "Step 1220/2142\n",
            "Step 1230/2142\n",
            "Step 1240/2142\n",
            "Step 1250/2142\n",
            "Step 1260/2142\n",
            "Step 1270/2142\n",
            "Step 1280/2142\n",
            "Step 1290/2142\n",
            "Step 1300/2142\n",
            "Step 1310/2142\n",
            "Step 1320/2142\n",
            "Step 1330/2142\n",
            "Step 1340/2142\n",
            "Step 1350/2142\n",
            "Step 1360/2142\n",
            "Step 1370/2142\n",
            "Step 1380/2142\n",
            "Step 1390/2142\n",
            "Step 1400/2142\n",
            "Step 1410/2142\n",
            "Step 1420/2142\n",
            "Step 1430/2142\n",
            "Step 1440/2142\n",
            "Step 1450/2142\n",
            "Step 1460/2142\n",
            "Step 1470/2142\n",
            "Step 1480/2142\n",
            "Step 1490/2142\n",
            "Step 1500/2142\n",
            "Step 1510/2142\n",
            "Step 1520/2142\n",
            "Step 1530/2142\n",
            "Step 1540/2142\n",
            "Step 1550/2142\n",
            "Step 1560/2142\n",
            "Step 1570/2142\n",
            "Step 1580/2142\n",
            "Step 1590/2142\n",
            "Step 1600/2142\n",
            "Step 1610/2142\n",
            "Step 1620/2142\n",
            "Step 1630/2142\n",
            "Step 1640/2142\n",
            "Step 1650/2142\n",
            "Step 1660/2142\n",
            "Step 1670/2142\n",
            "Step 1680/2142\n",
            "Step 1690/2142\n",
            "Step 1700/2142\n",
            "Step 1710/2142\n",
            "Step 1720/2142\n",
            "Step 1730/2142\n",
            "Step 1740/2142\n",
            "Step 1750/2142\n",
            "Step 1760/2142\n",
            "Step 1770/2142\n",
            "Step 1780/2142\n",
            "Step 1790/2142\n",
            "Step 1800/2142\n",
            "Step 1810/2142\n",
            "Step 1820/2142\n",
            "Step 1830/2142\n",
            "Step 1840/2142\n",
            "Step 1850/2142\n",
            "Step 1860/2142\n",
            "Step 1870/2142\n",
            "Step 1880/2142\n",
            "Step 1890/2142\n",
            "Step 1900/2142\n",
            "Step 1910/2142\n",
            "Step 1920/2142\n",
            "Step 1930/2142\n",
            "Step 1940/2142\n",
            "Step 1950/2142\n",
            "Step 1960/2142\n",
            "Step 1970/2142\n",
            "Step 1980/2142\n",
            "Step 1990/2142\n",
            "Step 2000/2142\n",
            "Step 2010/2142\n",
            "Step 2020/2142\n",
            "Step 2030/2142\n",
            "Step 2040/2142\n",
            "Step 2050/2142\n",
            "Step 2060/2142\n",
            "Step 2070/2142\n",
            "Step 2080/2142\n",
            "Step 2090/2142\n",
            "Step 2100/2142\n",
            "Step 2110/2142\n",
            "Step 2120/2142\n",
            "Step 2130/2142\n",
            "Step 2140/2142\n",
            "Epoch 2: Loss=26.6589, Accuracy=0.9961\n",
            "\n",
            "🟢 Début Epoch 3\n",
            "Step 0/2142\n",
            "Step 10/2142\n",
            "Step 20/2142\n",
            "Step 30/2142\n",
            "Step 40/2142\n",
            "Step 50/2142\n",
            "Step 60/2142\n",
            "Step 70/2142\n",
            "Step 80/2142\n",
            "Step 90/2142\n",
            "Step 100/2142\n",
            "Step 110/2142\n",
            "Step 120/2142\n",
            "Step 130/2142\n",
            "Step 140/2142\n",
            "Step 150/2142\n",
            "Step 160/2142\n",
            "Step 170/2142\n",
            "Step 180/2142\n",
            "Step 190/2142\n",
            "Step 200/2142\n",
            "Step 210/2142\n",
            "Step 220/2142\n",
            "Step 230/2142\n",
            "Step 240/2142\n",
            "Step 250/2142\n",
            "Step 260/2142\n",
            "Step 270/2142\n",
            "Step 280/2142\n",
            "Step 290/2142\n",
            "Step 300/2142\n",
            "Step 310/2142\n",
            "Step 320/2142\n",
            "Step 330/2142\n",
            "Step 340/2142\n",
            "Step 350/2142\n",
            "Step 360/2142\n",
            "Step 370/2142\n",
            "Step 380/2142\n",
            "Step 390/2142\n",
            "Step 400/2142\n",
            "Step 410/2142\n",
            "Step 420/2142\n",
            "Step 430/2142\n",
            "Step 440/2142\n",
            "Step 450/2142\n",
            "Step 460/2142\n",
            "Step 470/2142\n",
            "Step 480/2142\n",
            "Step 490/2142\n",
            "Step 500/2142\n",
            "Step 510/2142\n",
            "Step 520/2142\n",
            "Step 530/2142\n",
            "Step 540/2142\n",
            "Step 550/2142\n",
            "Step 560/2142\n",
            "Step 570/2142\n",
            "Step 580/2142\n",
            "Step 590/2142\n",
            "Step 600/2142\n",
            "Step 610/2142\n",
            "Step 620/2142\n",
            "Step 630/2142\n",
            "Step 640/2142\n",
            "Step 650/2142\n",
            "Step 660/2142\n",
            "Step 670/2142\n",
            "Step 680/2142\n",
            "Step 690/2142\n",
            "Step 700/2142\n",
            "Step 710/2142\n",
            "Step 720/2142\n",
            "Step 730/2142\n",
            "Step 740/2142\n",
            "Step 750/2142\n",
            "Step 760/2142\n",
            "Step 770/2142\n",
            "Step 780/2142\n",
            "Step 790/2142\n",
            "Step 800/2142\n",
            "Step 810/2142\n",
            "Step 820/2142\n",
            "Step 830/2142\n",
            "Step 840/2142\n",
            "Step 850/2142\n",
            "Step 860/2142\n",
            "Step 870/2142\n",
            "Step 880/2142\n",
            "Step 890/2142\n",
            "Step 900/2142\n",
            "Step 910/2142\n",
            "Step 920/2142\n",
            "Step 930/2142\n",
            "Step 940/2142\n",
            "Step 950/2142\n",
            "Step 960/2142\n",
            "Step 970/2142\n",
            "Step 980/2142\n",
            "Step 990/2142\n",
            "Step 1000/2142\n",
            "Step 1010/2142\n",
            "Step 1020/2142\n",
            "Step 1030/2142\n",
            "Step 1040/2142\n",
            "Step 1050/2142\n",
            "Step 1060/2142\n",
            "Step 1070/2142\n",
            "Step 1080/2142\n",
            "Step 1090/2142\n",
            "Step 1100/2142\n",
            "Step 1110/2142\n",
            "Step 1120/2142\n",
            "Step 1130/2142\n",
            "Step 1140/2142\n",
            "Step 1150/2142\n",
            "Step 1160/2142\n",
            "Step 1170/2142\n",
            "Step 1180/2142\n",
            "Step 1190/2142\n",
            "Step 1200/2142\n",
            "Step 1210/2142\n",
            "Step 1220/2142\n",
            "Step 1230/2142\n",
            "Step 1240/2142\n",
            "Step 1250/2142\n",
            "Step 1260/2142\n",
            "Step 1270/2142\n",
            "Step 1280/2142\n",
            "Step 1290/2142\n",
            "Step 1300/2142\n",
            "Step 1310/2142\n",
            "Step 1320/2142\n",
            "Step 1330/2142\n",
            "Step 1340/2142\n",
            "Step 1350/2142\n",
            "Step 1360/2142\n",
            "Step 1370/2142\n",
            "Step 1380/2142\n",
            "Step 1390/2142\n",
            "Step 1400/2142\n",
            "Step 1410/2142\n",
            "Step 1420/2142\n",
            "Step 1430/2142\n",
            "Step 1440/2142\n",
            "Step 1450/2142\n",
            "Step 1460/2142\n",
            "Step 1470/2142\n",
            "Step 1480/2142\n",
            "Step 1490/2142\n",
            "Step 1500/2142\n",
            "Step 1510/2142\n",
            "Step 1520/2142\n",
            "Step 1530/2142\n",
            "Step 1540/2142\n",
            "Step 1550/2142\n",
            "Step 1560/2142\n",
            "Step 1570/2142\n",
            "Step 1580/2142\n",
            "Step 1590/2142\n",
            "Step 1600/2142\n",
            "Step 1610/2142\n",
            "Step 1620/2142\n",
            "Step 1630/2142\n",
            "Step 1640/2142\n",
            "Step 1650/2142\n",
            "Step 1660/2142\n",
            "Step 1670/2142\n",
            "Step 1680/2142\n",
            "Step 1690/2142\n",
            "Step 1700/2142\n",
            "Step 1710/2142\n",
            "Step 1720/2142\n",
            "Step 1730/2142\n",
            "Step 1740/2142\n",
            "Step 1750/2142\n",
            "Step 1760/2142\n",
            "Step 1770/2142\n",
            "Step 1780/2142\n",
            "Step 1790/2142\n",
            "Step 1800/2142\n",
            "Step 1810/2142\n",
            "Step 1820/2142\n",
            "Step 1830/2142\n",
            "Step 1840/2142\n",
            "Step 1850/2142\n",
            "Step 1860/2142\n",
            "Step 1870/2142\n",
            "Step 1880/2142\n",
            "Step 1890/2142\n",
            "Step 1900/2142\n",
            "Step 1910/2142\n",
            "Step 1920/2142\n",
            "Step 1930/2142\n",
            "Step 1940/2142\n",
            "Step 1950/2142\n",
            "Step 1960/2142\n",
            "Step 1970/2142\n",
            "Step 1980/2142\n",
            "Step 1990/2142\n",
            "Step 2000/2142\n",
            "Step 2010/2142\n",
            "Step 2020/2142\n",
            "Step 2030/2142\n",
            "Step 2040/2142\n",
            "Step 2050/2142\n",
            "Step 2060/2142\n",
            "Step 2070/2142\n",
            "Step 2080/2142\n",
            "Step 2090/2142\n",
            "Step 2100/2142\n",
            "Step 2110/2142\n",
            "Step 2120/2142\n",
            "Step 2130/2142\n",
            "Step 2140/2142\n",
            "Epoch 3: Loss=20.1628, Accuracy=0.9970\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(label_encoder.classes_)\n",
        "model = NewsClassifier(num_classes).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(config.epochs):\n",
        "  print(f\"\\n🟢 Début Epoch {epoch + 1}\")\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  for step, batch in enumerate(train_loader):\n",
        "    if step % 10 == 0:\n",
        "      print(f\"Step {step}/{len(train_loader)}\")\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    outputs = model(input_ids, attention_mask)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "    all_preds.extend(preds.detach().cpu().numpy())\n",
        "    all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "  acc = accuracy_score(all_labels, all_preds)\n",
        "  wandb.log({\"train_loss\": total_loss / len(train_loader), \"train_accuracy\": acc})\n",
        "  print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, Accuracy={acc:.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gnu7_hvCZkuu"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "W8lnG8P6ZpB2",
        "outputId": "e9240ff2-c6c1-491b-b88d-2661d61bdcaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Rapport de classification :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "politicsNews       1.00      0.99      1.00      2256\n",
            "   worldnews       0.99      1.00      1.00      2028\n",
            "\n",
            "    accuracy                           1.00      4284\n",
            "   macro avg       1.00      1.00      1.00      4284\n",
            "weighted avg       1.00      1.00      1.00      4284\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.99627</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bert-base-title-text</strong> at: <a href='https://wandb.ai/kenkleven50-lbv/news-subject-classification/runs/va8ls42n' target=\"_blank\">https://wandb.ai/kenkleven50-lbv/news-subject-classification/runs/va8ls42n</a><br> View project at: <a href='https://wandb.ai/kenkleven50-lbv/news-subject-classification' target=\"_blank\">https://wandb.ai/kenkleven50-lbv/news-subject-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250520_175927-va8ls42n/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "report = classification_report(all_labels, all_preds, target_names=label_encoder.classes_, output_dict=True)\n",
        "wandb.log({\"eval_accuracy\": accuracy_score(all_labels, all_preds)})\n",
        "wandb.log({\"classification_report\": report})\n",
        "print(\"\\n📊 Rapport de classification :\")\n",
        "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifoMd4XF9TK6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "output_dir = \"saved_model_bert\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
